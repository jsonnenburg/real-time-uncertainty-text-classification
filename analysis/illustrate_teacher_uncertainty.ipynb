{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.data.robustness_study.bert_data_preprocessing import bert_preprocess\n",
    "from src.models.bert_model import AleatoricMCDropoutBERT, create_bert_config\n",
    "from src.utils.loss_functions import null_loss, bayesian_binary_crossentropy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T13:55:21.775810Z",
     "start_time": "2024-03-15T13:55:08.665007Z"
    }
   },
   "id": "68ce5aadcab5c039"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# load teacher model and processed data\n",
    "with open(os.path.join('../out/bert_teacher/final_e3_lr2_hd020_ad020_cd030/model/config.json'), 'r') as f:\n",
    "    teacher_config = json.load(f)\n",
    "    \n",
    "config = create_bert_config(teacher_config['hidden_dropout_prob'],\n",
    "                            teacher_config['attention_probs_dropout_prob'],\n",
    "                            teacher_config['classifier_dropout'])\n",
    "\n",
    "# initialize teacher model\n",
    "teacher = AleatoricMCDropoutBERT(config=config, custom_loss_fn=bayesian_binary_crossentropy)\n",
    "checkpoint_path = os.path.join('out/bert_teacher/final_e3_lr2_hd020_ad020_cd030/model', 'cp-{epoch:02d}.ckpt')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    print(\"Loading weights from\", checkpoint_dir)\n",
    "    teacher.load_weights(latest_checkpoint).expect_partial()\n",
    "\n",
    "teacher.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=teacher_config['learning_rate']),\n",
    "        loss={'classifier': bayesian_binary_crossentropy, 'log_variance': null_loss},\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "        run_eagerly=True\n",
    "    )\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(os.path.join('../data/robustness_study/preprocessed/train.csv'), sep='\\t', index_col=0)\n",
    "\n",
    "# randomly choose one sequence\n",
    "sample = df.sample(1)\n",
    "\n",
    "# preprocess sampled sequence\n",
    "input_ids, attention_masks, labels = bert_preprocess(sample)\n",
    "sample_preprocessed = tf.data.Dataset.from_tensor_slices((\n",
    "    sample['text'].values,\n",
    "    {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks\n",
    "    },\n",
    "    labels\n",
    ")).batch(1)\n",
    "\n",
    "# for this sequence, first compute epistemic uncertainty (simple MC dropout sampling)\n",
    "# to illustrate, use 50 samples\n",
    "\n",
    "\n",
    "# then compute aleatoric uncertainty (from mean prediction and log variance)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:35:45.552613Z",
     "start_time": "2024-03-15T14:35:43.089413Z"
    }
   },
   "id": "b8895210e36ea523"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "def illustrate_uncertainties(model, data: tf.data.Dataset, m: int = 50):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo Dropout Transfer Sampling on a given model and dataset.\n",
    "\n",
    "    This method generates an augmented dataset with additional uncertain labels\n",
    "    created by MC Dropout and aleatoric uncertainty sampling.\n",
    "\n",
    "    :param  model: The TensorFlow model to use for sampling.\n",
    "    :param  data: Data to be used for sampling. Each element should be a tuple (features, labels).\n",
    "    :param  m: Number of MC Dropout iterations.\n",
    "    :param k: Number of aleatoric uncertainty samples per MC iteration.\n",
    "    :return: df: Augmented dataset with original features, labels, and uncertain labels.\n",
    "    \"\"\"\n",
    "\n",
    "    text, features, labels = next(iter(data))\n",
    "    all_logits = []\n",
    "    all_log_variances = []\n",
    "    for i in range(m):\n",
    "        print('sampling epistemic uncertainty: {}/{}'.format(i, m))\n",
    "        rand_seed = random.randint(0, 2 ** 32 - 1)\n",
    "        tf.random.set_seed(rand_seed)\n",
    "        outputs = model(features, training=True)\n",
    "        logits = outputs.logits\n",
    "        log_variances = outputs.log_variances\n",
    "        all_logits.append(logits)\n",
    "        all_log_variances.append(log_variances)\n",
    "\n",
    "    mu_t = tf.stack(all_logits, axis=0)  # shape is (m, batch_size, num_classes)\n",
    "    all_log_variances = tf.stack(all_log_variances, axis=0)\n",
    "    \n",
    "    mean_logits = tf.reduce_mean(mu_t, axis=0)\n",
    "\n",
    "    sigma_hat = tf.sqrt(tf.exp(all_log_variances))\n",
    "    sigma_tilde = tf.reduce_mean(sigma_hat, axis=0)\n",
    "\n",
    "    return mu_t, mean_logits, sigma_tilde"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:02:11.895436Z",
     "start_time": "2024-03-15T15:02:11.893427Z"
    }
   },
   "id": "c62273948a20000c"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling epistemic uncertainty: 0/25\n",
      "sampling epistemic uncertainty: 1/25\n",
      "sampling epistemic uncertainty: 2/25\n",
      "sampling epistemic uncertainty: 3/25\n",
      "sampling epistemic uncertainty: 4/25\n",
      "sampling epistemic uncertainty: 5/25\n",
      "sampling epistemic uncertainty: 6/25\n",
      "sampling epistemic uncertainty: 7/25\n",
      "sampling epistemic uncertainty: 8/25\n",
      "sampling epistemic uncertainty: 9/25\n",
      "sampling epistemic uncertainty: 10/25\n",
      "sampling epistemic uncertainty: 11/25\n",
      "sampling epistemic uncertainty: 12/25\n",
      "sampling epistemic uncertainty: 13/25\n",
      "sampling epistemic uncertainty: 14/25\n",
      "sampling epistemic uncertainty: 15/25\n",
      "sampling epistemic uncertainty: 16/25\n",
      "sampling epistemic uncertainty: 17/25\n",
      "sampling epistemic uncertainty: 18/25\n",
      "sampling epistemic uncertainty: 19/25\n",
      "sampling epistemic uncertainty: 20/25\n",
      "sampling epistemic uncertainty: 21/25\n",
      "sampling epistemic uncertainty: 22/25\n",
      "sampling epistemic uncertainty: 23/25\n",
      "sampling epistemic uncertainty: 24/25\n"
     ]
    }
   ],
   "source": [
    "m = 25\n",
    "k = 25\n",
    "mcd_sample, mean_logit, mean_std_dev_logit = illustrate_uncertainties(teacher, sample_preprocessed, m)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:04:47.625282Z",
     "start_time": "2024-03-15T15:02:53.561636Z"
    }
   },
   "id": "af7ed45c9f8138a6"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "mean_std_dev_logit = mean_std_dev"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:04:53.143228Z",
     "start_time": "2024-03-15T15:04:53.137845Z"
    }
   },
   "id": "7204e3e78dfb5842"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# sample k times from the aleatoric uncertainty\n",
    "# k std normal samples\n",
    "eps = tf.random.normal((k, 1, 1), mean=0, stddev=1)\n",
    "\n",
    "y_aleatoric_logits = mean_logits + (mean_std_dev_logit * eps)  # y_t should be (m, batch_size, num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:08:21.777264Z",
     "start_time": "2024-03-15T15:08:21.579247Z"
    }
   },
   "id": "ee4336d61f6c092c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c273948bb6680e2"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[0.567637]], dtype=float32), array([[0.74959713]], dtype=float32))"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sigmoid(mean_logits).numpy(), tf.sigmoid(mean_std_dev_logit).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:09:28.383894Z",
     "start_time": "2024-03-15T15:09:28.363154Z"
    }
   },
   "id": "41c842aca90bec69"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "y_aleatoric_probs = tf.sigmoid(y_aleatoric_logits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:08:22.193498Z",
     "start_time": "2024-03-15T15:08:22.188365Z"
    }
   },
   "id": "43bce068cbac251a"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "y_epistemic_probs = tf.sigmoid(mcd_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:08:31.467337Z",
     "start_time": "2024-03-15T15:08:31.456343Z"
    }
   },
   "id": "be51f3d72e0f677a"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:19:54.107992Z",
     "start_time": "2024-03-15T15:19:54.104702Z"
    }
   },
   "id": "79cbbc6cfa23cac9"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "total_uncertainty = (y_epistemic_probs[:, 0, 0] + y_aleatoric_probs[:, 0, 0]).numpy()\n",
    "\n",
    "# normalize into 0, 1 interval\n",
    "total_uncertainty = (total_uncertainty - total_uncertainty.min()) / (total_uncertainty.max() - total_uncertainty.min())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:25:21.722993Z",
     "start_time": "2024-03-15T15:25:21.717511Z"
    }
   },
   "id": "50c43177b6d0fe46"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.histplot(y_epistemic_probs[:, 0, 0], label='Epistemic Uncertainty', alpha=0.5, binwidth=0.05)\n",
    "sns.histplot(y_aleatoric_probs[:, 0, 0], label='Aleatoric Uncertainty', alpha=0.5, color='orange', binwidth=0.05)\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/plot_illustration_teacher_uncertainty_sample.pdf')\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:39:06.638985Z",
     "start_time": "2024-03-15T15:39:06.516357Z"
    }
   },
   "id": "28f5057277a0d5e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c309953acc8fbca5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
